{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating NLP methods of finding text similarities and anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing (NLP) methods for finding text similarities and performing anomaly detection are essential for various applications, including information retrieval, recommendation systems, and fraud detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Similarity with NLP:\n",
    "Text similarity is the process of determining how similar two pieces of text are. Here's an example of using NLP methods for text similarity:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cosine similarity measures the cosine of the angle between two non-zero vectors in a multi-dimensional space. It is commonly used for text similarity tasks. You can use popular NLP libraries such as spaCy or scikit-learn in Python to calculate cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names: ['artificial' 'future' 'intelligence' 'is' 'learning' 'machine' 'of'\n",
      " 'shape' 'technology' 'the' 'will']\n",
      "TF-IDF Matrix:\n",
      " [[0.         0.33471228 0.         0.47042643 0.47042643 0.47042643\n",
      "  0.         0.         0.33471228 0.33471228 0.        ]\n",
      " [0.39166832 0.27867523 0.39166832 0.         0.         0.\n",
      "  0.39166832 0.39166832 0.27867523 0.27867523 0.39166832]]\n",
      "Cosine Similarity:  0.2798280652432878\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Machine Learning is the future technology.\"\n",
    "text2 = \"Artificial Intelligence will shape the future of technology.\"\n",
    "\n",
    "# Create a TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
    "\n",
    "# Display the feature names and TF-IDF matrix\n",
    "print(\"Feature Names:\", vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_similarity = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])\n",
    "print(\"Cosine Similarity: \", cosine_similarity[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF matrix represents the weight of each word in each text. \n",
    "The cosine similarity score reflects the overall similarity between the two TF-IDF vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity ranges from -1 to 1:\n",
    "\n",
    "- 1: The vectors are identical. This measures the cosine of the angle between the vectors, with smaller angles (closer to 1) indicating higher similarity.\n",
    "- 0: The vectors are orthogonal (no similarity).\n",
    "- -1: The vectors are diametrically opposite.\n",
    "- A score of 0.2798 suggests that the two texts share some common terms but are not very similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF Vector Calculation:\n",
    "- Term Frequency (TF): How often a word appears in a document.\n",
    "- Inverse Document Frequency (IDF): Measures the importance of a word across the corpus. Common words have lower IDF scores.\n",
    "- TF-IDF: Combines TF and IDF to give a weight to each word, reflecting its importance in the document relative to the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practical Implications\n",
    "A moderate cosine similarity score can be interpreted in various ways:\n",
    "\n",
    "- Text Comparison: Helps in identifying related documents or passages.\n",
    "- Recommendation Systems: Suggests similar items based on textual descriptions.\n",
    "- Information Retrieval: Enhances search engines by ranking results based on textual relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings (Word2Vec, GloVe):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are dense vector representations of words. You can calculate text similarity using pre-trained word embeddings and cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.11.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus\n",
    "sentences = [\n",
    "    [\"machine\", \"learning\", \"is\", \"the\", \"future\", \"technology\"],\n",
    "    [\"artificial\", \"intelligence\", \"will\", \"shape\", \"the\", \"future\", \"of\", \"technology\"]\n",
    "]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model:\n",
    "\n",
    "- sentences: A list of tokenized sentences (corpus) used to train the Word2Vec model.\n",
    "- Word2Vec: Trains the model on the given corpus with specified parameters like vector_size, window, min_count, and workers.\n",
    "- model.save(\"word2vec.model\"): Saves the trained model to a file named word2vec.model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity:  0.108892545\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained Word2Vec model\n",
    "model = Word2Vec.load(\"Word2Vec.model\")\n",
    "\n",
    "# Calculate cosine similarity between two sentences\n",
    "similarity = cosine_similarity([model.wv['machine'], model.wv['learning']],[model.wv['artificial'], model.wv['intelligence']])\n",
    "\n",
    "print(\"Cosine Similarity: \", similarity[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection with NLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly detection in text involves identifying unusual or outlier patterns in textual data. Here's an example of using NLP for anomaly detection:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection with TF-IDF:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use TF-IDF to detect anomalies by identifying documents that significantly differ from the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies: [ 1  1 -1]\n"
     ]
    }
   ],
   "source": [
    "documents = [\"Normal text\", \"Another normal text\", \"Anomaly detected here\"]\n",
    "\n",
    "# Create TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Detect anomalies using isolation forest\n",
    "clf = IsolationForest(contamination=.2)\n",
    "clf.fit(tfidf_matrix.toarray())\n",
    "anomalies = clf.predict(tfidf_matrix.toarray())\n",
    "\n",
    "print(\"Anomalies:\", anomalies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IsolationForest is an anomaly detection algorithm.\n",
    "- contamination=0.2 specifies that approximately 20% of the data is expected to be anomalies.\n",
    "- By adjusting the contamination parameter, you can control the sensitivity of the anomaly detection process, making it suitable for different use cases and datasets.\n",
    "- 1 indicates normal points.\n",
    "- -1 indicates anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practical Implications\n",
    "This approach can be used to automatically detect anomalous documents or texts in a corpus, which can be useful in various applications such as:\n",
    "\n",
    "- Fraud Detection: Identifying unusual transactions or behaviors.\n",
    "- Quality Control: Detecting defective products or processes.\n",
    "- Security: Identifying malicious activities or intrusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Autoencoder for Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more advanced anomaly detection in sequences (e.g., identifying abnormal text patterns in a sequence of sentences), you can use recurrent neural networks (RNN) like LSTM autoencoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sequence_length - 3\n",
      "sequences - [[2, 1, 3], [2, 1, 4], [5, 1]]\n",
      "pad_sequences - [[2 1 3]\n",
      " [2 1 4]\n",
      " [5 1 0]]\n",
      "One-hot encoded sequences - [[[0. 0. 1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0.]]\n",
      "\n",
      " [[0. 0. 1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 1.]\n",
      "  [0. 1. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0.]]]\n",
      "Epoch 1/100\n",
      "3/3 [==============================] - 5s 6ms/step - loss: 1.7934\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.7844\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.7751\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 1.7674\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 1.7587\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.7488\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 1.7381\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.7273\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.7134\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.6991\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 1.6844\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.6619\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.6408\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 1.6107\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 1.5839\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 1.5496\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 1.5084\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 1.4623\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.4167\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 1.3631\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.3045\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 1.2419\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 1.1774\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 1.1135\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 1.0446\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.9887\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.9220\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.8610\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.7929\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.7295\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6723\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.6193\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.5718\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.5241\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.4767\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4404\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4025\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3684\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3414\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3139\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.2887\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2609\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2428\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.2199\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2016\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1834\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1734\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.1480\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.1327\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.1250\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.1064\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0937\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0823\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0726\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0620\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0549\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0501\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0444\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0396\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0354\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0320\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0296\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0269\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0242\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0227\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0205\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0192\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0180\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.0172\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0160\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0153\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0144\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0135\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0129\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0124\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0120\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0113\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0110\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0105\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0101\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0097\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0094\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0091\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0087\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0085\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0082\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0079\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0077\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0075\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0073\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0070\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0069\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0067\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.0065\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0063\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0061\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0060\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0058\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0057\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0056\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "reconstructed_sequences: [[[1.30393546e-05 3.87220527e-03 9.95564878e-01 2.54486891e-04\n",
      "   1.73383523e-04 1.21991368e-04]\n",
      "  [1.30118792e-06 9.94792879e-01 2.21638381e-03 2.81412504e-03\n",
      "   1.65128571e-04 1.01742116e-05]\n",
      "  [1.49423067e-06 2.40800646e-03 6.20907740e-05 9.95316386e-01\n",
      "   2.21084896e-03 1.23757411e-06]]\n",
      "\n",
      " [[1.73645021e-05 2.88591813e-03 9.96542275e-01 6.89739682e-05\n",
      "   2.94837402e-04 1.90629871e-04]\n",
      "  [1.05504896e-05 9.96033728e-01 2.01901305e-03 1.27916297e-04\n",
      "   1.78053020e-03 2.82085730e-05]\n",
      "  [1.87580183e-04 2.55188742e-03 1.38041069e-04 1.91855431e-03\n",
      "   9.95197713e-01 6.14242117e-06]]\n",
      "\n",
      " [[9.90302302e-04 8.77377205e-03 1.12563861e-03 6.26878027e-05\n",
      "   1.08006905e-04 9.88939583e-01]\n",
      "  [3.18366359e-03 9.92618144e-01 1.30772794e-04 1.48200879e-05\n",
      "   2.58165619e-05 4.02682973e-03]\n",
      "  [9.96138394e-01 3.20327398e-03 1.11107852e-06 1.72701766e-06\n",
      "   2.41159742e-05 6.31424831e-04]]]\n",
      "Anomaly scores: [5.9652079e-06 4.2620673e-06 1.7111575e-05]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample sequences\n",
    "sequences = [\"Normal sentence 1\", \"Normal sentence 2\", \"Anomaly sentence\"]\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "max_sequence_length = max(len(s.split()) for s in sequences)\n",
    "print(f\"max_sequence_length - {max_sequence_length}\")\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sequences)\n",
    "sequences_tokenized = tokenizer.texts_to_sequences(sequences)\n",
    "print(f\"sequences - {sequences_tokenized}\")\n",
    "\n",
    "sequences_padded = pad_sequences(sequences_tokenized, maxlen=max_sequence_length, padding='post')\n",
    "print(f\"pad_sequences - {sequences_padded}\")\n",
    "\n",
    "# One-hot encode the padded sequences\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 for the <OOV> token\n",
    "sequences_one_hot = np.array([to_categorical(seq, num_classes=vocab_size) for seq in sequences_padded])\n",
    "print(f\"One-hot encoded sequences - {sequences_one_hot}\")\n",
    "\n",
    "# Create LSTM autoencoder\n",
    "input_seq = Input(shape=(max_sequence_length, vocab_size))\n",
    "encoded = LSTM(64)(input_seq)\n",
    "decoded = RepeatVector(max_sequence_length)(encoded)\n",
    "decoded = LSTM(64, return_sequences=True)(decoded)\n",
    "output_seq = TimeDistributed(Dense(vocab_size, activation='softmax'))(decoded)\n",
    "\n",
    "autoencoder = Model(input_seq, output_seq)\n",
    "autoencoder.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(sequences_one_hot, sequences_one_hot, epochs=100, batch_size=1)\n",
    "\n",
    "# Detect anomalies by measuring reconstruction error\n",
    "reconstructed_sequences = autoencoder.predict(sequences_one_hot)\n",
    "print('reconstructed_sequences:', reconstructed_sequences)\n",
    "\n",
    "anomaly_scores = np.mean(np.square(sequences_one_hot - reconstructed_sequences), axis=(1, 2))\n",
    "print('Anomaly scores:', anomaly_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Output\n",
    "Epochs 1-100:\n",
    "\n",
    "Training is carried out over 100 epochs, which means the entire training dataset is passed forward and backward through the neural network 100 times.\n",
    "The loss decreases consistently over the epochs, indicating that the model is learning and improving its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Value:\n",
    "\n",
    "The loss value measures how well the model's predictions match the actual data. A lower loss indicates better model performance.\n",
    "The consistent reduction in loss over epochs signifies that the model is successfully learning to reconstruct the sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly Scores:\n",
    "\n",
    "- Anomaly scores: [2.728174e-06, 6.158271e-06, 1.629073e-05]\n",
    "- These scores represent the reconstruction error for each sequence.\n",
    "- Lower scores indicate sequences that are well-reconstructed by the autoencoder and are considered normal.\n",
    "- Higher scores indicate sequences that deviate from the expected pattern and are considered anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model shows good learning progression, with the loss decreasing significantly over 100 epochs.\n",
    "Anomaly scores are very low, indicating that the model has learned to reconstruct the input sequences well, and there are no significant anomalies in the provided data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
